{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from tester import dump_classifier_and_data\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## declare simple data types\n",
    "\n",
    "adj_types = [\"unscaled\", \"unscaled - custom\", \"scaled\", \"scaled - custom\"]\n",
    "\n",
    "text_shape = \"The original dataset has {0} rows and {1} columns\"\n",
    "text_class_dist = \"The original dataset has {0} POI records and {1} non-POI records\"\n",
    "text_nan = \"The following table is the count of NaN values in the original dataset.\"\n",
    "text_final_clf = \"The best performing classifier was the {0} using {1} features with an F1-score of {2}.\"\n",
    "text_other_clf = \"{0} classifier had an F1-score of {1}\"\n",
    "text_dataset_size = (\"The original dataset was resampled to add 50 POI records, to assist with the imbalanced classes. After being split into training and testing data\"\n",
    "                     \" the training dataset had {0} rows and the testing dataset had {1} rows.\")\n",
    "text_comp_perf = (\"The dataset with the highest performance used {2} features. With this dataset, my custom feature's best classifier had an F1-score of {0},\"\n",
    "                  \" while the standard feature's best classifier had an F1-score of {1}.\")\n",
    "\n",
    "text_final_features = (\"The best performing classifier used {0} features. \"\n",
    "                       \"Below are the most important features and their relative importance according to the ExtraTreeClassifier. {1}\"\n",
    "                       )\n",
    "text_gt_10_features = \"Note: Because there are more than 10 final features, I am only including the first 10 below.\"\n",
    "\n",
    "trn_set_size = 0\n",
    "tst_set_size = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## declare objects\n",
    "\n",
    "datasets = []\n",
    "clf_models = []\n",
    "f1_score = []\n",
    "cr_dict = {}\n",
    "\n",
    "df_nan = None\n",
    "\n",
    "desc_variables = {\"text_shape\": text_shape, \"text_class_dist\": text_class_dist, \"df_nan\": df_nan,\n",
    "                  \"text_nan\": text_nan, \"trn_set_size\": trn_set_size, \"tst_set_size\": tst_set_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "# process_data function accepts a few different arguments, allowing me to scale, resample, and filter the features\n",
    "# https://elitedatascience.com/imbalanced-classes\n",
    "\n",
    "\n",
    "def process_data(data, label_column='poi', scale=0, rsmpl=0, feature_list=None):\n",
    "    # Resample first, if needed\n",
    "    # https://stackoverflow.com/questions/52735334/python-pandas-resample-dataset-to-have-balanced-classes\n",
    "\n",
    "    if rsmpl:\n",
    "\n",
    "        df_majority = data[data[label_column] == 0]\n",
    "        df_minority = data[data[label_column] == 1]\n",
    "\n",
    "        # Upsample minority class\n",
    "        df_minority_upsampled = utils.resample(df_minority.copy(),\n",
    "                                               n_samples=50,\n",
    "                                               replace=True     # sample with replacement\n",
    "                                               )\n",
    "\n",
    "        # because we are resampling, and we need to export the data to a dictionary, we need to add some\n",
    "        # randomness to the resampled names\n",
    "        new_index_names = []\n",
    "        for i, j in enumerate(df_minority_upsampled.index):\n",
    "            new_index_names.append(j + '_' + str(np.random.random()))\n",
    "\n",
    "        df_minority_upsampled.index = new_index_names\n",
    "\n",
    "        # Combine majority class with upsampled minority class\n",
    "        data = pd.concat([df_majority, df_minority_upsampled, df_minority])\n",
    "\n",
    "    # separate features from labels\n",
    "\n",
    "    features = data.drop(columns=[label_column])\n",
    "    labels = data[label_column]\n",
    "\n",
    "    if feature_list:\n",
    "        features = features[feature_list]\n",
    "\n",
    "# adjusted scaler to use StandardScaler, as found in this post on sklearn\n",
    "# https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "    if scale:\n",
    "        for x in features.columns:\n",
    "            if is_numeric_dtype(features[x]):\n",
    "                features[x] = preprocessing.StandardScaler().fit_transform(\n",
    "                    features[x].values.reshape(-1, 1))\n",
    "\n",
    "    features = features.astype(float)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, stratify=labels)\n",
    "\n",
    "    desc_variables[\"trn_set_size\"] = len(X_train)\n",
    "    desc_variables[\"tst_set_size\"] = len(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "\n",
    "# Feature Importance with Extra Trees Classifier\n",
    "\n",
    "\n",
    "def get_best_features(X_train, X_test, y_train, percentile):\n",
    "    et_clf = ExtraTreesClassifier(n_estimators=100)\n",
    "    et_clf.fit(X_train, y_train)\n",
    "\n",
    "    df_feature_importance = pd.DataFrame(\n",
    "        et_clf.feature_importances_, index=X_train.columns)\n",
    "    df_feature_importance = df_feature_importance.sort_values(\n",
    "        by=0, ascending=False)\n",
    "    df_feature_importance = df_feature_importance[df_feature_importance[0] >= percentile].reset_index(\n",
    "    )\n",
    "\n",
    "    feature_list = df_feature_importance['index']\n",
    "\n",
    "    X_train = X_train[feature_list]\n",
    "    X_test = X_test[feature_list]\n",
    "\n",
    "    return X_train, X_test, df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_features_and_append_datasets(adj_type):\n",
    "    if 'custom' in adj_type:\n",
    "        perc = .0\n",
    "    else:\n",
    "        perc = .06\n",
    "\n",
    "    if 'scale' in adj_type:\n",
    "        __X_train, __X_test, best_features = get_best_features(\n",
    "            X_train_scale, X_test_scale, y_train_scale, perc)\n",
    "\n",
    "        datasets.append((__X_train, __X_test, y_train_scale, y_test_scale,\n",
    "                         best_features, adj_type, df_scale))\n",
    "    else:\n",
    "        __X_train, __X_test, best_features = get_best_features(\n",
    "            X_train_std, X_test_std, y_train_std, perc)\n",
    "\n",
    "        datasets.append((__X_train, __X_test, y_train_std, y_test_std,\n",
    "                         best_features, adj_type, df_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_datasets():\n",
    "    for X_train, X_test, y_train, y_test, feature_list, adj_type, df_loop in datasets:\n",
    "        for clf, params in clf_models:\n",
    "\n",
    "            if params:\n",
    "                Grid_CV = GridSearchCV(clf, params, cv=5, iid=True)\n",
    "                gv = Grid_CV.fit(X_train, y_train)\n",
    "                clf.set_params(**gv.best_params_)\n",
    "\n",
    "            clf.fit(X_train, y_train)\n",
    "            clf.score(X_test, y_test)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "\n",
    "            f1_score.append((clf, df_loop, feature_list, adj_type,\n",
    "                                  metrics.f1_score(y_test, y_pred)))\n",
    "\n",
    "            cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "            if adj_type not in cr_dict:\n",
    "                cr_dict[adj_type] = {clf.__class__.__name__:cr}\n",
    "            else:\n",
    "                cr_dict[adj_type][clf.__class__.__name__] = cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_initial_data():\n",
    "\n",
    "    # Load the dictionary containing the dataset\n",
    "    # with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"r\"))\n",
    "\n",
    "    # Load the dictionary containing the processed text from the emails\n",
    "    # Code is in vectorize_text.py - heavily borrowed from the scripts used in the text learning module\n",
    "    # as well as the email_preprocess module\n",
    "    email_dict = pickle.load(open(\"email_text.pkl\", \"r\"))\n",
    "\n",
    "    # convert dictionary to dataframe for easier manipulation\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data_dict, orient=\"index\")\n",
    "\n",
    "    # adjust email dataset to format needed to join to df\n",
    "    for x, y in email_dict.items():\n",
    "        email_dict[x] = ''.join(y)\n",
    "\n",
    "    df_email_text = pd.DataFrame.from_dict(email_dict, orient=\"index\")\n",
    "\n",
    "    df_email_text.rename(index=str, columns={0: 'email_text'}, inplace=True)\n",
    "\n",
    "    # describe data\n",
    "    # Understanding the Dataset and Question\n",
    "\n",
    "    # Task 2: Remove outliers\n",
    "    # fillna did not work (likely due to NaN being strings, not np.nan), so use replace\n",
    "\n",
    "    df.replace(\"NaN\", np.nan, inplace=True)\n",
    "    df.poi = df.poi.astype(int)\n",
    "\n",
    "    if len(df[df.index == \"TOTAL\"].values):\n",
    "        df.drop(\"TOTAL\", inplace=True)\n",
    "\n",
    "    # Record initial data exploration variables, such as shape of data and number of NANs\n",
    "\n",
    "    x, y = df.shape\n",
    "\n",
    "    desc_variables[\"text_shape\"] = text_shape.format(x, y)\n",
    "\n",
    "    desc_variables[\"text_class_dist\"] = text_class_dist.format(\n",
    "        len(df[df.poi == 1].values), x - len(df[df.poi == 1].values))\n",
    "\n",
    "    desc_variables[\"df_nan\"] = pd.DataFrame(\n",
    "        df.isna().sum()).rename(index={0: 'NANs'})\n",
    "\n",
    "    # Create joined dataset\n",
    "\n",
    "    df = pd.merge(left=df, right=df_email_text, how='left',\n",
    "                  left_on='email_address', right_index=True).drop(columns=['email_address'])\n",
    "\n",
    "    # set up TfIdf Vectorizer for converting email text to features\n",
    "\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.50,\n",
    "                                 stop_words='english', max_features=5000)\n",
    "\n",
    "    email_transformed = vectorizer.fit_transform(df['email_text'].fillna(''))\n",
    "\n",
    "    df_email_transformed = pd.DataFrame(\n",
    "        email_transformed.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "    return df.join(df_email_transformed, rsuffix='et_').drop(columns=['email_text']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing sets\n",
    "# return df as well, since we modify columns and data if we resample, scale, or remove features\n",
    "\n",
    "df = process_initial_data()\n",
    "\n",
    "X_train_std, X_test_std, y_train_std, y_test_std, df_std = process_data(\n",
    "    df.copy(), rsmpl=1)\n",
    "X_train_scale, X_test_scale, y_train_scale, y_test_scale, df_scale = process_data(\n",
    "    df.copy(), rsmpl=1, scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in adj_types:\n",
    "    get_best_features_and_append_datasets(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all models to a list, with parameters for tuning if needed, to be fit and scored at the end\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "gb = GaussianNB()\n",
    "\n",
    "clf_models.append((gb, None))\n",
    "\n",
    "# SVM\n",
    "SVC_clf = SVC(kernel='rbf', C=1000, gamma='scale', random_state=12)\n",
    "\n",
    "parameters = {\"kernel\": ['rbf', 'poly', 'sigmoid'], \"C\": [.1,\n",
    "                                                          10, 100, 1000], \"gamma\": ['scale'], \"random_state\": [40]}\n",
    "\n",
    "clf_models.append((SVC_clf, parameters))\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "dt_clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "parameters = {\"min_samples_split\": [2, 3], \"max_features\": [\n",
    "    None, 2, 5, ], \"random_state\": [40]}\n",
    "\n",
    "clf_models.append((dt_clf, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/31421413/how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run feature tuning and scoring on all datasets in datasets list\n",
    "del f1_score[:]\n",
    "\n",
    "run_all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max by type - do this so we can compare custom features to standard features\n",
    "\n",
    "max_by_type = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "i = 0\n",
    "for __clf, __df, __features, __adj_type, __f1_score in f1_score:\n",
    "    if __f1_score > max_by_type[__adj_type][\"max\"]:\n",
    "        max_by_type[__adj_type][\"max\"] = __f1_score\n",
    "        max_by_type[__adj_type][\"max_i\"] = i\n",
    "        max_by_type[__adj_type][\"features\"] = __features\n",
    "        max_by_type[__adj_type][\"df\"] = __df\n",
    "        max_by_type[__adj_type][\"clf\"] = __clf\n",
    "    i += 1\n",
    "\n",
    "# get overall best performing classifier\n",
    "\n",
    "mx = 0\n",
    "mx_adj_type = \"\"\n",
    "\n",
    "for x, y in max_by_type.items():\n",
    "    if y.get(\"max\") > mx:\n",
    "        mx = y.get(\"max\")\n",
    "        mx_adj_type = x\n",
    "\n",
    "\n",
    "final_clf, final_df, final_features, final_adj_type, final_f1_score = f1_score[max_by_type.get(\n",
    "    mx_adj_type).get(\"max_i\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_feature_list = list(final_features['index'].values)\n",
    "if exp_feature_list[0] != 'poi':\n",
    "    exp_feature_list.insert(0, 'poi')\n",
    "\n",
    "dump_classifier_and_data(\n",
    "    final_clf, final_df.to_dict('index'), exp_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_final_clf = text_final_clf.format(final_clf.__class__.__name__,\n",
    "                                       final_adj_type, round(final_f1_score, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_adj_type = final_adj_type.split(' ')[0]\n",
    "\n",
    "custom_adj_type = final_adj_type.split(' ')[0] + ' - custom'\n",
    "\n",
    "custom_features_f1_score = f1_score[max_by_type.get(\n",
    "    custom_adj_type).get(\"max_i\")][4]\n",
    "\n",
    "unscaled_features_f1_score = f1_score[max_by_type.get(\n",
    "    standard_adj_type).get(\"max_i\")][4]\n",
    "\n",
    "text_comp_perf = text_comp_perf.format(round(custom_features_f1_score, 4), round(\n",
    "    unscaled_features_f1_score, 4), standard_adj_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset_size = text_dataset_size.format(\n",
    "    desc_variables[\"trn_set_size\"], desc_variables[\"tst_set_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(final_features) > 10:\n",
    "    text_final_features = text_final_features.format(\n",
    "        len(final_features), text_gt_10_features)\n",
    "    disp_final_features = final_features[:10]\n",
    "else:\n",
    "    text_final_features = text_final_features.format(len(final_features), '')\n",
    "    disp_final_features = final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_clf_perf = []\n",
    "\n",
    "for __clf, __df, __features, __adj_type, __f1_score in f1_score:\n",
    "    if __adj_type == final_adj_type and __clf is not final_clf:\n",
    "        other_clf_perf.append(text_other_clf.format(__clf.__class__.__name__,round(__f1_score,4)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_params = \"{0} classifier the parameters that were tuned were {1}\"\n",
    "\n",
    "all_clf_params = []\n",
    "\n",
    "for __clf, __params in clf_models:\n",
    "    if __params:\n",
    "         all_clf_params.append(text_clf_params.format(__clf.__class__.__name__,', '.join(__params.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## https://stackoverflow.com/questions/24988131/nested-dictionary-to-multiindex-dataframe-where-dictionary-keys-are-column-label\n",
    "\n",
    "reform = {(outerKey, innerKey, innerMostKey): values for outerKey, innerDict in \n",
    "          cr_dict.iteritems() for innerKey, innerMostDict in innerDict.iteritems()\n",
    "          for innerMostKey, values in innerMostDict.iteritems() if 'avg' not in innerMostKey}\n",
    "\n",
    "cr = pd.DataFrame().from_dict(reform,'index')\n",
    "\n",
    "cr.index = cr.index.set_names(['adj_type','clf','class'])\n",
    "\n",
    "cr = cr[['recall','precision','f1-score']]\n",
    "\n",
    "cr = cr.round(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "all_clf_params[0]": "SVC classifier the parameters that were tuned were kernel, C, random_state, gamma",
     "all_clf_params[1]": "DecisionTreeClassifier classifier the parameters that were tuned were min_samples_split, max_features, random_state",
     "cr[cr.index.get_level_values('class')=='1'].groupby('adj_type').mean()": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>recall</th>\n      <th>precision</th>\n      <th>f1-score</th>\n    </tr>\n    <tr>\n      <th>adj_type</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>scaled</th>\n      <td>0.705867</td>\n      <td>0.689667</td>\n      <td>0.614833</td>\n    </tr>\n    <tr>\n      <th>scaled - custom</th>\n      <td>0.980400</td>\n      <td>0.682300</td>\n      <td>0.783433</td>\n    </tr>\n    <tr>\n      <th>unscaled</th>\n      <td>0.745100</td>\n      <td>0.763867</td>\n      <td>0.718700</td>\n    </tr>\n    <tr>\n      <th>unscaled - custom</th>\n      <td>0.980400</td>\n      <td>0.655567</td>\n      <td>0.767833</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
     "cr[cr.index.get_level_values('class')=='1'].groupby('clf').mean()": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>recall</th>\n      <th>precision</th>\n      <th>f1-score</th>\n    </tr>\n    <tr>\n      <th>clf</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>DecisionTreeClassifier</th>\n      <td>0.970600</td>\n      <td>0.806675</td>\n      <td>0.880425</td>\n    </tr>\n    <tr>\n      <th>GaussianNB</th>\n      <td>0.602925</td>\n      <td>0.542900</td>\n      <td>0.437525</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>0.985300</td>\n      <td>0.743975</td>\n      <td>0.845650</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
     "cr[cr.index.get_level_values('class')=='1'].groupby(['adj_type','clf']).mean()": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>recall</th>\n      <th>precision</th>\n      <th>f1-score</th>\n    </tr>\n    <tr>\n      <th>adj_type</th>\n      <th>clf</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">scaled</th>\n      <th>DecisionTreeClassifier</th>\n      <td>1.0000</td>\n      <td>0.7727</td>\n      <td>0.8718</td>\n    </tr>\n    <tr>\n      <th>GaussianNB</th>\n      <td>0.1176</td>\n      <td>0.6667</td>\n      <td>0.2000</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>1.0000</td>\n      <td>0.6296</td>\n      <td>0.7727</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">scaled - custom</th>\n      <th>DecisionTreeClassifier</th>\n      <td>0.9412</td>\n      <td>0.8421</td>\n      <td>0.8889</td>\n    </tr>\n    <tr>\n      <th>GaussianNB</th>\n      <td>1.0000</td>\n      <td>0.3953</td>\n      <td>0.5667</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>1.0000</td>\n      <td>0.8095</td>\n      <td>0.8947</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">unscaled</th>\n      <th>DecisionTreeClassifier</th>\n      <td>1.0000</td>\n      <td>0.8500</td>\n      <td>0.9189</td>\n    </tr>\n    <tr>\n      <th>GaussianNB</th>\n      <td>0.2941</td>\n      <td>0.7143</td>\n      <td>0.4167</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>0.9412</td>\n      <td>0.7273</td>\n      <td>0.8205</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">unscaled - custom</th>\n      <th>DecisionTreeClassifier</th>\n      <td>0.9412</td>\n      <td>0.7619</td>\n      <td>0.8421</td>\n    </tr>\n    <tr>\n      <th>GaussianNB</th>\n      <td>1.0000</td>\n      <td>0.3953</td>\n      <td>0.5667</td>\n    </tr>\n    <tr>\n      <th>SVC</th>\n      <td>1.0000</td>\n      <td>0.8095</td>\n      <td>0.8947</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
     "desc_variables[\"df_nan\"]": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>salary</th>\n      <td>51</td>\n    </tr>\n    <tr>\n      <th>to_messages</th>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>deferral_payments</th>\n      <td>107</td>\n    </tr>\n    <tr>\n      <th>total_payments</th>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>exercised_stock_options</th>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>bonus</th>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>restricted_stock</th>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>shared_receipt_with_poi</th>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>restricted_stock_deferred</th>\n      <td>128</td>\n    </tr>\n    <tr>\n      <th>total_stock_value</th>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>expenses</th>\n      <td>51</td>\n    </tr>\n    <tr>\n      <th>loan_advances</th>\n      <td>142</td>\n    </tr>\n    <tr>\n      <th>from_messages</th>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>other</th>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>from_this_person_to_poi</th>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>poi</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>director_fees</th>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>deferred_income</th>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>long_term_incentive</th>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>email_address</th>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>from_poi_to_this_person</th>\n      <td>59</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
     "desc_variables[\"text_class_dist\"]": "The original dataset has 18 POI records and 127 non-POI records",
     "desc_variables[\"text_nan\"]": "The following table is the count of NaN values in the original dataset.",
     "desc_variables[\"text_shape\"]": "The original dataset has 145 rows and 21 columns",
     "disp_final_features": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>exercised_stock_options</td>\n      <td>0.107956</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bonus</td>\n      <td>0.091950</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>total_stock_value</td>\n      <td>0.083077</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>long_term_incentive</td>\n      <td>0.070065</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>salary</td>\n      <td>0.066010</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>expenses</td>\n      <td>0.065296</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
     "final_adj_type": "unscaled",
     "other_clf_perf[0]": "GaussianNB classifier had an F1-score of 0.4167",
     "other_clf_perf[1]": "SVC classifier had an F1-score of 0.8205",
     "text_comp_perf": "&quot;The dataset with the highest performance used unscaled features. With this dataset, my custom feature&#39;s best classifier had an F1-score of 0.8947, while the standard feature&#39;s best classifier had an F1-score of 0.9189.&quot;",
     "text_dataset_size": "The original dataset was resampled to add 50 POI records, to assist with the imbalanced classes. After being split into training and testing data the training dataset had 146 rows and the testing dataset had 49 rows.",
     "text_final_clf": "The best performing classifier was the DecisionTreeClassifier using unscaled features with an F1-score of 0.9189.",
     "text_final_features": "The best performing classifier used 6 features. Below are the most important features and their relative importance according to the ExtraTreeClassifier. "
    }
   },
   "source": [
    "## Files\n",
    "parse_out_email_text.py - from Text Learning module\n",
    "\n",
    "vectorize_text.py - based on script from Text Learning module\n",
    "\n",
    "email_text.pkl - output of vectorize_text.py\n",
    "\n",
    "poi_id.py - final project\n",
    "\n",
    "tester.py - test script (Note: I could not get this to run, there were multiple errors with StratifiedShuffleSplit that I did not want to take the time to try and troubleshoot. However, it did run to the point of pulling in my data and attempting to score it, so that was enough validation for me)\n",
    "\n",
    "\n",
    "## Questions\n",
    "\n",
    "##### Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "The goal of this project is to use the given data that we have about the Enron employees financial and email data to try and determine which were/would have been persons of interest in the investigation. Machine learning is a very useful tool here, because there is a lot of data (especially in the emails) and a lot of patterns - the financial data alone has roughly 20 features. It is hard for humans to find patterns out of this much data, so we turn to machine learning to help.\n",
    "\n",
    "There are multiple ways to use the data to determine if someone is a POI - I attempted two of them. The first is to use this financial + email dataset to see if there are any patterns in the financial or email numbers that correlate with someone being a POI. Another way of looking at this is to use the actual email text to find patters - when I did this in my vectorize_text.py script, I was getting a score of around 90, with a recall of .5 and a precision of .89 - which was a much simpler and quicker way of getting high accuracy compared to using this dataset.\n",
    "\n",
    "The only outlier I noticed was the \"Total\" record, which had numbers way higher than any other record. Once I removed this record, I didn't see any other large outliers, so I left the rest of the potential outliers in.\n",
    "\n",
    "##### Data exploration\n",
    "\n",
    "{{desc_variables[\"text_shape\"]}}\n",
    "\n",
    "{{desc_variables[\"text_class_dist\"]}}\n",
    "\n",
    "{{desc_variables[\"text_nan\"]}}\n",
    "\n",
    "{{desc_variables[\"df_nan\"]}}\n",
    "\n",
    "##### What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n",
    "\n",
    "I used an ExtraTreeClassifier to determine the features to use - I used a 60% percentile for the \"standard\" features (data + financial) and used a 0% percentile for my custom features - as I found that these were consistently marked as 0.0 performance.\n",
    "\n",
    "I ran three different classifiers against four different datasets - each combination of the standard features vs standard plus my custom features, as well as scaled vs unscaled. The best performing classifier ended up using the {{final_adj_type}} dataset.\n",
    "\n",
    "My custom features were generated using the text from the emails that each person sent - based on what was in the emails_by_address folder. I then ran these through a TfIdf vectorizer. My assumption was that this should give quite a bit more power to the classifier, as POIs would probably use some words more often than non-POIs. As we'll see below, the datasets with the custom features have a better average Recall score than the dataset that uses only the standard features.\n",
    "\n",
    "##### The below stats are for the last run completed.\n",
    "\n",
    "{{text_comp_perf}}\n",
    "\n",
    "{{text_final_features}}\n",
    "\n",
    "{{disp_final_features}}\n",
    "\n",
    "\n",
    "##### What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "\n",
    "I found that the SVC and the Decision Tree Classifier gave me the best balanced score, on average. This being said, I wrote my script to return the classifier that gave me the best balanced score, so I never chose any one algorith per se, I let the metrics choose for me. Besides the SVC and Decision Tree, I also used a Gaussian Niave Bayes classifier, however this consistently performed much lower than the others.\n",
    "\n",
    "##### The below stats are for the last run completed.\n",
    "\n",
    "{{text_final_clf}}\n",
    "\n",
    "With this same set of features, the {{other_clf_perf[0]}}, and the {{other_clf_perf[1]}}.\n",
    "\n",
    "##### What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "\n",
    "Tuning an algorithm is to find what parameters for that algorithm give you the best performance. Without tuning your algorithms, you can have much worse performance.\n",
    "\n",
    "With the exception of the Niave Bayes classifier, I tuned all of my classifiers using the GridSearchCV estimator, passing in a defined list of parameters and potential values. For the {{all_clf_params[0]}}, and for the {{all_clf_params[1]}}.\n",
    "\n",
    "\n",
    "##### What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]\n",
    "\n",
    "Validation is how you test that your model works on actual data. The most important thing here is to have, at the least, a separate training and testing data set. If you do your testing on your training data, you are not going to see how your model performs on real data, because it was already trained on the same data you are using for testing.\n",
    "\n",
    "I validated my data by splitting it into separate training and testing data sets. {{text_dataset_size}}\n",
    "\n",
    "##### Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "There are two evaluation metrics I pay a lot of attention to, precision and recall. The reason I chose to ignore the general accuracy score is that it does not give a great representation when you have imbalanced classes, which is what we had in this dataset. However, the F1 score is another great evaluation metric, especially for imbalanced. It is the harmonic mean of precision and recall. Based on my research, this is a very good metric to use when classifying imbalanced classes with few positive records, like what we have here.\n",
    "\n",
    "Precision is the ratio of how many times the model __accurately__ predicted the record was a POI record, compared to all of the times it predicted the the record was a POI record in total. For example, if it predicted 3 non-POI records were POI records, and it predicted 7 POI records were POI records, this would be a precision rate of .7.\n",
    "\n",
    "Recall is a bit more important in this model, in my opinion. Recall is the ratio of how many times, when encountering a POI record, did it __correctly__ classify that it was a POI record.\n",
    "\n",
    "Below I have the average for recall, precision, and F1 for the POI class, grouped by both the dataset and by the classifier, as well as by both.\n",
    "\n",
    "\n",
    "#### The below stats are for the last run completed.\n",
    "\n",
    "##### By feature type (unscaled/scaled, standard/custom)\n",
    "\n",
    "{{cr[cr.index.get_level_values('class')=='1'].groupby('adj_type').mean()}}\n",
    "\n",
    "##### By classifier\n",
    "\n",
    "{{cr[cr.index.get_level_values('class')=='1'].groupby('clf').mean()}}\n",
    "\n",
    "##### By both\n",
    "\n",
    "{{cr[cr.index.get_level_values('class')=='1'].groupby(['adj_type','clf']).mean()}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
